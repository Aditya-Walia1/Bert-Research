{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyNonxf9tnYJ6RJSNKPmMpuX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aditya-Walia1/Bert-Research/blob/main/D4_Bertweet_Optimised.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna\n"
      ],
      "metadata": {
        "id": "1rBi7GMpUnGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "q8hRequ9SESL",
        "outputId": "b777056b-e81c-4df1-9692-3dcb6188a5ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.21.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.3.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.0+cu121)\n",
            "Requirement already satisfied: emoji==0.6.0 in /usr/local/lib/python3.10/dist-packages (0.6.0)\n",
            "Collecting optuna\n",
            "  Downloading optuna-3.6.1-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.5)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.5)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.13.2-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.8.2-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.32)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna)\n",
            "  Downloading Mako-1.3.5-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (3.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Downloading optuna-3.6.1-py3-none-any.whl (380 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m380.1/380.1 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.13.2-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.0/233.0 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.8.2-py3-none-any.whl (11 kB)\n",
            "Downloading Mako-1.3.5-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Mako, colorlog, alembic, optuna\n",
            "Successfully installed Mako-1.3.5 alembic-1.13.2 colorlog-6.8.2 optuna-3.6.1\n",
            "                                               title  \\\n",
            "0  Ben Stein Calls Out 9th Circuit Court: Committ...   \n",
            "1  Trump drops Steve Bannon from National Securit...   \n",
            "2  Puerto Rico expects U.S. to lift Jones Act shi...   \n",
            "3   OOPS: Trump Just Accidentally Confirmed He Le...   \n",
            "4  Donald Trump heads for Scotland to reopen a go...   \n",
            "\n",
            "                                                text       subject  \\\n",
            "0  21st Century Wire says Ben Stein, reputable pr...       US_News   \n",
            "1  WASHINGTON (Reuters) - U.S. President Donald T...  politicsNews   \n",
            "2  (Reuters) - Puerto Rico Governor Ricardo Rosse...  politicsNews   \n",
            "3  On Monday, Donald Trump once again embarrassed...          News   \n",
            "4  GLASGOW, Scotland (Reuters) - Most U.S. presid...  politicsNews   \n",
            "\n",
            "                  date  label  \n",
            "0    February 13, 2017      0  \n",
            "1       April 5, 2017       1  \n",
            "2  September 27, 2017       1  \n",
            "3         May 22, 2017      0  \n",
            "4       June 24, 2016       1  \n",
            "Index(['title', 'text', 'subject', 'date', 'label'], dtype='object')\n",
            "Encoded labels: [0 1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2024-08-28 09:31:48,617] A new study created in memory with name: no-name-7d56a2ad-9db2-4f91-8e48-bef726a123be\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='6735' max='6735' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [6735/6735 16:18, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.000200</td>\n",
              "      <td>0.004142</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.000500</td>\n",
              "      <td>0.001831</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000086</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-08-28 09:49:14,677] Trial 0 finished with value: 1.0 and parameters: {'learning_rate': 3.296808487005066e-05, 'num_train_epochs': 3, 'per_device_train_batch_size': 16}. Best is trial 0 with value: 1.0.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2246' max='2246' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2246/2246 09:46, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>0.000068</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000584</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-08-28 10:00:08,098] Trial 1 finished with value: 0.9998886414253898 and parameters: {'learning_rate': 2.2332540380095077e-05, 'num_train_epochs': 2, 'per_device_train_batch_size': 32}. Best is trial 0 with value: 1.0.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3369' max='3369' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3369/3369 14:39, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001504</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000007</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-08-28 10:15:54,773] Trial 2 finished with value: 1.0 and parameters: {'learning_rate': 1.1293934716541742e-05, 'num_train_epochs': 3, 'per_device_train_batch_size': 32}. Best is trial 0 with value: 1.0.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='8980' max='8980' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [8980/8980 21:22, Epoch 4/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>0.004413</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>0.004142</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001225</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.049900</td>\n",
              "      <td>0.001297</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-08-28 10:38:23,832] Trial 3 finished with value: 0.9998886414253898 and parameters: {'learning_rate': 3.5693012747017084e-05, 'num_train_epochs': 4, 'per_device_train_batch_size': 16}. Best is trial 0 with value: 1.0.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='4490' max='4490' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [4490/4490 10:41, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001804</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001832</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-08-28 10:50:12,151] Trial 4 finished with value: 0.9998886414253898 and parameters: {'learning_rate': 1.645337205192521e-05, 'num_train_epochs': 2, 'per_device_train_batch_size': 16}. Best is trial 0 with value: 1.0.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='17960' max='17960' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [17960/17960 25:24, Epoch 4/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.356100</td>\n",
              "      <td>0.318054</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.663600</td>\n",
              "      <td>0.686929</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.000500</td>\n",
              "      <td>0.010772</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.117300</td>\n",
              "      <td>0.005262</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-08-28 11:16:43,603] Trial 5 finished with value: 0.9994432071269488 and parameters: {'learning_rate': 3.904778462342574e-05, 'num_train_epochs': 4, 'per_device_train_batch_size': 8}. Best is trial 0 with value: 1.0.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='5615' max='5615' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [5615/5615 24:22, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.000200</td>\n",
              "      <td>0.005236</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.000300</td>\n",
              "      <td>0.005636</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.000200</td>\n",
              "      <td>0.006077</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.028400</td>\n",
              "      <td>0.005149</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>0.005209</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-08-28 11:42:13,147] Trial 6 finished with value: 0.9994432071269488 and parameters: {'learning_rate': 1.7412580419909152e-05, 'num_train_epochs': 5, 'per_device_train_batch_size': 32}. Best is trial 0 with value: 1.0.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2246' max='2246' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2246/2246 09:49, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.033600</td>\n",
              "      <td>0.005524</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.005674</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-08-28 11:53:10,649] Trial 7 finished with value: 0.9994432071269488 and parameters: {'learning_rate': 1.3209601002400526e-05, 'num_train_epochs': 2, 'per_device_train_batch_size': 32}. Best is trial 0 with value: 1.0.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='8980' max='8980' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [8980/8980 12:50, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.006835</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.005069</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-08-28 12:07:08,082] Trial 8 finished with value: 0.999554565701559 and parameters: {'learning_rate': 1.5558682443526842e-05, 'num_train_epochs': 2, 'per_device_train_batch_size': 8}. Best is trial 0 with value: 1.0.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='4950' max='5615' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [4950/5615 21:11 < 02:50, 3.89 it/s, Epoch 4.41/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.007800</td>\n",
              "      <td>0.014946</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.024500</td>\n",
              "      <td>0.008839</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.144100</td>\n",
              "      <td>1.270069</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.698000</td>\n",
              "      <td>0.683880</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Install necessary libraries\n",
        "!pip install transformers datasets scikit-learn pandas torch emoji==0.6.0 optuna\n",
        "\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import optuna\n",
        "import pickle\n",
        "\n",
        "# Load the datasets (assuming the CSV files are named 'Fake.csv' and 'True.csv')\n",
        "df_fake = pd.read_csv('Fake.csv')\n",
        "df_real = pd.read_csv('True.csv')\n",
        "\n",
        "# Add a 'label' column to each dataset\n",
        "df_fake['label'] = 0  # Fake news\n",
        "df_real['label'] = 1  # Real news\n",
        "\n",
        "# Combine the datasets\n",
        "df_combined = pd.concat([df_fake, df_real], ignore_index=True)\n",
        "\n",
        "# Shuffle the dataset to mix fake and real news\n",
        "df_combined = df_combined.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "# Save the combined dataset to a new CSV file (optional)\n",
        "df_combined.to_csv('combined_news.csv', index=False)\n",
        "\n",
        "# Display the first few rows to understand the structure\n",
        "print(df_combined.head())\n",
        "\n",
        "# Inspect the column names to identify the text and label columns\n",
        "print(df_combined.columns)\n",
        "\n",
        "# Update these variables with the actual column names in your dataset\n",
        "text_column = 'text'  # Assuming the text column is named 'text'\n",
        "label_column = 'label'  # The correct column name for the labels\n",
        "\n",
        "# Encode the labels as integers\n",
        "label_encoder = LabelEncoder()\n",
        "df_combined[label_column] = label_encoder.fit_transform(df_combined[label_column])\n",
        "\n",
        "# Verify the encoding (optional)\n",
        "print(\"Encoded labels:\", label_encoder.classes_)\n",
        "\n",
        "# Preprocess the dataset\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=False,\n",
        "            padding='max_length',\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "            truncation=True\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Load BERTweet tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained('vinai/bertweet-base', use_fast=False)\n",
        "model = AutoModelForSequenceClassification.from_pretrained('vinai/bertweet-base', num_labels=len(df_combined[label_column].unique()))\n",
        "\n",
        "# Split the dataset into train and validation sets\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    df_combined[text_column].values,\n",
        "    df_combined[label_column].values,\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Define max token length\n",
        "MAX_LEN = 128\n",
        "\n",
        "# Create train and validation datasets\n",
        "train_dataset = CustomDataset(train_texts, train_labels, tokenizer, MAX_LEN)\n",
        "val_dataset = CustomDataset(val_texts, val_labels, tokenizer, MAX_LEN)\n",
        "\n",
        "# Define the objective function for Optuna\n",
        "def objective(trial):\n",
        "    # Define the hyperparameters to tune\n",
        "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 5e-5, log=True)\n",
        "    num_train_epochs = trial.suggest_int('num_train_epochs', 2, 5)\n",
        "    per_device_train_batch_size = trial.suggest_categorical('per_device_train_batch_size', [8, 16, 32])\n",
        "\n",
        "    # Update the TrainingArguments with Optuna's suggestions\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir='./results',\n",
        "        num_train_epochs=num_train_epochs,\n",
        "        per_device_train_batch_size=per_device_train_batch_size,\n",
        "        per_device_eval_batch_size=16,\n",
        "        warmup_steps=500,\n",
        "        weight_decay=0.01,\n",
        "        logging_dir='./logs',\n",
        "        logging_steps=10,\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        learning_rate=learning_rate\n",
        "    )\n",
        "\n",
        "    # Initialize the Trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        tokenizer=tokenizer\n",
        "    )\n",
        "\n",
        "    # Train and evaluate the model\n",
        "    trainer.train()\n",
        "    eval_result = trainer.evaluate()\n",
        "\n",
        "    # Use accuracy as the metric for optimization\n",
        "    preds = trainer.predict(val_dataset).predictions.argmax(-1)\n",
        "    accuracy = accuracy_score(val_labels, preds)\n",
        "\n",
        "    return accuracy\n",
        "\n",
        "# Run the Optuna study\n",
        "study = optuna.create_study(direction='maximize')\n",
        "study.optimize(objective, n_trials=10)  # You can increase n_trials for a more thorough search\n",
        "\n",
        "# Print the best hyperparameters\n",
        "print(\"Best hyperparameters: \", study.best_trial.params)\n",
        "\n",
        "# Train the final model with the best hyperparameters\n",
        "best_params = study.best_trial.params\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=best_params['num_train_epochs'],\n",
        "    per_device_train_batch_size=best_params['per_device_train_batch_size'],\n",
        "    per_device_eval_batch_size=16,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=best_params['learning_rate']\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "# Train the final model\n",
        "trainer.train()\n",
        "\n",
        "# Evaluate the final model\n",
        "eval_result = trainer.evaluate()\n",
        "\n",
        "# Predictions for validation data\n",
        "predictions = trainer.predict(val_dataset)\n",
        "preds = torch.argmax(torch.tensor(predictions.predictions), axis=1)\n",
        "\n",
        "# Convert label classes to strings (necessary for classification_report)\n",
        "target_names = [str(label) for label in label_encoder.classes_]\n",
        "\n",
        "# Calculate and print detailed classification metrics\n",
        "accuracy = accuracy_score(val_labels, preds)\n",
        "conf_matrix = confusion_matrix(val_labels, preds)\n",
        "class_report = classification_report(val_labels, preds, target_names=target_names)\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
        "print(\"Classification Report:\\n\", class_report)\n",
        "\n",
        "# Optional: Save the label encoder for later use\n",
        "with open('label_encoder.pkl', 'wb') as le_file:\n",
        "    pickle.dump(label_encoder, le_file)\n"
      ]
    }
  ]
}